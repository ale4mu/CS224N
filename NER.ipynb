{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predict a word is a \"location\" or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['we', 'always', 'come', 'to', 'paris'], ['the', 'professor', 'is', 'from', 'australia'], ['i', 'live', 'in', 'stanford'], ['he', 'comes', 'from', 'taiwan'], ['the', 'capital', 'of', 'turkey', 'is', 'ankara']]\n",
      "[[0, 0, 0, 0, 1], [0, 0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1, 0, 1]]\n"
     ]
    }
   ],
   "source": [
    "corpus = [\n",
    "          \"We always come to Paris\",\n",
    "          \"The professor is from Australia\",\n",
    "          \"I live in Stanford\",\n",
    "          \"He comes from Taiwan\",\n",
    "          \"The capital of Turkey is Ankara\"\n",
    "         ]\n",
    "\n",
    "def preprocess_sentence(sentence):\n",
    "    return sentence.lower().split()\n",
    "\n",
    "train_sentences = [preprocess_sentence(sent) for sent in corpus]\n",
    "print(train_sentences)\n",
    "\n",
    "locations = set([\"paris\", \"australia\", \"stanford\", \"taiwan\", \"ankara\",\"turkey\"]) # 标注地点\n",
    "train_labels = [[1 if word in locations else 0 for word in sent]for sent in train_sentences]\n",
    "print(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'from', 'to', 'paris', 'stanford', 'we', 'come', 'always', 'he', 'australia', 'the', 'comes', 'capital', 'ankara', '<pad>', '<unk>', 'is', 'live', 'turkey', 'professor', 'taiwan', 'i', 'in', 'of'}\n"
     ]
    }
   ],
   "source": [
    "voc = set(w for s in train_sentences for w in s) # 词汇表\n",
    "voc.add(\"<unk>\") #for unknown words\n",
    "voc.add(\"<pad>\") #for words at the beginning and end\n",
    "print(voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<pad>', '<pad>', 'we', 'always', 'come', 'to', 'paris', '<pad>', '<pad>']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to make every window the same length\n",
    "def pad_window(sentence, window_size, pad_token=\"<pad>\"):\n",
    "  window = [pad_token] * window_size\n",
    "  return window + sentence + window\n",
    "\n",
    "# Show padding example\n",
    "window_size = 2\n",
    "pad_window(train_sentences[0], window_size=window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<pad>', '<unk>', 'always', 'ankara', 'australia', 'capital', 'come', 'comes', 'from', 'he', 'i', 'in', 'is', 'live', 'of', 'paris', 'professor', 'stanford', 'taiwan', 'the', 'to', 'turkey', 'we']\n",
      "{'<pad>': 0, '<unk>': 1, 'always': 2, 'ankara': 3, 'australia': 4, 'capital': 5, 'come': 6, 'comes': 7, 'from': 8, 'he': 9, 'i': 10, 'in': 11, 'is': 12, 'live': 13, 'of': 14, 'paris': 15, 'professor': 16, 'stanford': 17, 'taiwan': 18, 'the': 19, 'to': 20, 'turkey': 21, 'we': 22}\n"
     ]
    }
   ],
   "source": [
    "ix_to_word = sorted(list(voc)) # 给出索引，能够找到对应的单词\n",
    "print(ix_to_word)\n",
    "word_to_ix={word:ix for ix,word in enumerate(ix_to_word)} # 给出单词，能找到对应的索引\n",
    "print(word_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence is: ['we', 'always', 'come', 'to', 'kuwait']\n",
      "Going from words to indices: [22, 2, 6, 20, 1]\n",
      "Going from indices to words: ['we', 'always', 'come', 'to', '<unk>']\n"
     ]
    }
   ],
   "source": [
    "def convert_token_to_indices(sentence,word_to_ix):\n",
    "    return [word_to_ix.get(word,word_to_ix[\"<unk>\"]) for word in sentence]# if word isn't in the voc -> <unk>\n",
    "\n",
    "example_sentence = [\"we\", \"always\", \"come\", \"to\", \"kuwait\"]\n",
    "example_indices = convert_token_to_indices(example_sentence, word_to_ix) # kuwait是<unk>\n",
    "restored_example = [ix_to_word[ind] for ind in example_indices]\n",
    "\n",
    "# 验证成功实现了单词和索引间的相互转换\n",
    "print(f\"Original sentence is: {example_sentence}\")\n",
    "print(f\"Going from words to indices: {example_indices}\")\n",
    "print(f\"Going from indices to words: {restored_example}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[22, 2, 6, 20, 15], [19, 16, 12, 8, 4], [10, 13, 11, 17], [9, 7, 8, 18], [19, 5, 14, 21, 12, 3]]\n"
     ]
    }
   ],
   "source": [
    "example_padded_indices = [convert_token_to_indices(s,word_to_ix)for s in train_sentences] # 应用到我们的训练样本上\n",
    "print(example_padded_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[ 1.8665e+00,  2.6193e-01,  1.9000e-01, -8.2790e-01, -8.4364e-01],\n",
       "         [-7.2985e-01,  1.1892e+00,  7.0656e-02,  3.8087e-01,  1.5866e+00],\n",
       "         [ 7.5231e-01,  1.4707e+00, -3.1502e-01,  5.9916e-01, -1.0444e+00],\n",
       "         [-1.6701e-01, -1.0700e+00, -7.9754e-02,  2.3688e-01, -2.2711e+00],\n",
       "         [ 1.7645e+00,  4.8368e-01,  3.2452e-01,  2.9931e-01,  5.5431e-01],\n",
       "         [-8.2392e-02,  1.5849e+00, -1.4372e+00,  6.4310e-01, -4.4925e-01],\n",
       "         [ 3.2781e-01,  4.7918e-01,  1.6109e-03,  1.2416e+00,  5.9712e-01],\n",
       "         [-5.4880e-01, -6.2648e-01,  1.4517e+00, -6.3594e-01,  5.9755e-01],\n",
       "         [-2.3429e-01,  2.1966e-01,  1.4106e+00, -1.7326e+00, -1.2497e+00],\n",
       "         [ 4.4186e-01,  7.7119e-02, -1.8368e-02, -1.2357e+00,  6.3652e-01],\n",
       "         [ 8.3299e-01,  1.0178e+00, -3.4494e-01, -6.1611e-01,  7.2718e-01],\n",
       "         [ 4.0414e-01, -7.7575e-01,  1.6117e-01, -2.2091e-01,  8.8417e-01],\n",
       "         [ 6.6395e-02,  7.1163e-01, -2.5933e-01, -1.0118e-01, -3.9772e-01],\n",
       "         [-2.9812e-01,  2.0919e+00, -1.8390e-01, -2.9943e+00,  6.8118e-01],\n",
       "         [-6.0759e-01,  2.6211e+00, -7.3424e-02, -3.2094e-01,  2.2550e-01],\n",
       "         [-8.0984e-02, -3.7217e-01,  2.0939e+00,  8.1632e-01,  4.8497e-01],\n",
       "         [ 1.0313e-02,  4.1138e-01, -2.3396e-01,  7.2815e-01, -3.0956e-01],\n",
       "         [-2.7327e-01, -1.4762e+00, -1.2522e+00, -7.6865e-01, -7.4606e-01],\n",
       "         [-9.3754e-01, -9.6228e-01, -9.0248e-01,  2.2045e+00,  3.4712e-01],\n",
       "         [ 3.9878e-01, -6.7995e-01,  2.2840e-02, -1.4140e-01, -7.4228e-01],\n",
       "         [ 1.1488e+00, -1.0442e+00, -8.5532e-03, -1.0223e+00,  3.8714e-01],\n",
       "         [ 1.9074e+00,  2.3500e-01, -3.2416e-01, -8.1578e-01, -1.6337e-01],\n",
       "         [ 6.1660e-02,  1.0588e+00,  5.2224e-01, -8.1634e-01,  8.7874e-01]],\n",
       "        requires_grad=True)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "dim = 5\n",
    "embeds = nn.Embedding(len(voc),dim) # 随机初始化\n",
    "list(embeds.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0810, -0.3722,  2.0939,  0.8163,  0.4850],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "tensor([[-0.0810, -0.3722,  2.0939,  0.8163,  0.4850],\n",
      "        [-0.1670, -1.0700, -0.0798,  0.2369, -2.2711]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Get the embedding for the word Paris\n",
    "index = word_to_ix[\"paris\"]\n",
    "index_tensor = torch.tensor(index, dtype=torch.long) # nn.Embedding的输入索引必须是torch.long类型\n",
    "paris_embed = embeds(index_tensor)\n",
    "print(paris_embed)\n",
    "\n",
    "# We can also get multiple embeddings at once\n",
    "index_paris = word_to_ix[\"paris\"]\n",
    "index_ankara = word_to_ix[\"ankara\"]\n",
    "indices = [index_paris, index_ankara]\n",
    "indices_tensor = torch.tensor(indices, dtype=torch.long)\n",
    "embeddings = embeds(indices_tensor)\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from functools import partial\n",
    "\n",
    "def custom_collate_fn(batch,window_size,word_to_ix):\n",
    "    x,y = zip(*batch) #split the value and label\n",
    "    x = [pad_window(sent,window_size) for sent in x]\n",
    "    x = [convert_token_to_indices(sent,word_to_ix) for sent in x]\n",
    "    pad_token_ix = word_to_ix[\"<pad>\"]\n",
    "    x = [torch.LongTensor(xi) for xi in x] # 同样，embedding的索引是torch.long类型\n",
    "    x_p = nn.utils.rnn.pad_sequence(x,batch_first=True,padding_value=pad_token_ix) # 补齐成相同的长度\n",
    "\n",
    "    # 对标签做相同的操作\n",
    "    lengths = [len(label) for label in y]\n",
    "    lenghts = torch.LongTensor(lengths)\n",
    "    y = [torch.LongTensor(yi) for yi in y]\n",
    "    y_p = nn.utils.rnn.pad_sequence(y, batch_first=True, padding_value=0)\n",
    "\n",
    "    return x_p,y_p,lenghts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "Batched Input:\n",
      "tensor([[ 0,  0, 19,  5, 14, 21, 12,  3,  0,  0],\n",
      "        [ 0,  0, 19, 16, 12,  8,  4,  0,  0,  0]])\n",
      "Batched Labels:\n",
      "tensor([[0, 0, 0, 1, 0, 1],\n",
      "        [0, 0, 0, 0, 1, 0]])\n",
      "Batched Lengths:\n",
      "tensor([6, 5])\n",
      "\n",
      "Iteration 1\n",
      "Batched Input:\n",
      "tensor([[ 0,  0, 10, 13, 11, 17,  0,  0,  0],\n",
      "        [ 0,  0, 22,  2,  6, 20, 15,  0,  0]])\n",
      "Batched Labels:\n",
      "tensor([[0, 0, 0, 1, 0],\n",
      "        [0, 0, 0, 0, 1]])\n",
      "Batched Lengths:\n",
      "tensor([4, 5])\n",
      "\n",
      "Iteration 2\n",
      "Batched Input:\n",
      "tensor([[ 0,  0,  9,  7,  8, 18,  0,  0]])\n",
      "Batched Labels:\n",
      "tensor([[0, 0, 0, 1]])\n",
      "Batched Lengths:\n",
      "tensor([4])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = list(zip(train_sentences,train_labels))\n",
    "batch_size = 2\n",
    "shuffle = True\n",
    "collate_fn = partial(custom_collate_fn,window_size=window_size,word_to_ix=word_to_ix)#only one parameter in DataLoader,so use partial \n",
    "\n",
    "loader = DataLoader(data,batch_size=batch_size,shuffle=shuffle,collate_fn=collate_fn)\n",
    "counter = 0\n",
    "for batched_x, batched_y, batched_lengths in loader:\n",
    "  print(f\"Iteration {counter}\")\n",
    "  print(\"Batched Input:\")\n",
    "  print(batched_x)\n",
    "  print(\"Batched Labels:\")\n",
    "  print(batched_y)\n",
    "  print(\"Batched Lengths:\")\n",
    "  print(batched_lengths)\n",
    "  print(\"\")\n",
    "  counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Tensor: \n",
      "tensor([[ 0,  0,  9,  7,  8, 18,  0,  0]])\n",
      "\n",
      "Windows: \n",
      "tensor([[[ 0,  0,  9,  7,  8],\n",
      "         [ 0,  9,  7,  8, 18],\n",
      "         [ 9,  7,  8, 18,  0],\n",
      "         [ 7,  8, 18,  0,  0]]])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Original Tensor: \")\n",
    "print(batched_x)\n",
    "print(\"\")\n",
    "\n",
    "# Create the 2 * 2 + 1 chunks\n",
    "chunk = batched_x.unfold(1, window_size*2 + 1, 1)# step = 1\n",
    "print(f\"Windows: \")\n",
    "print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordWindowClassifer(nn.Module):\n",
    "    def __init__(self,parameters,voc_size,pad_ix=0):\n",
    "        super(WordWindowClassifer,self).__init__()\n",
    "        #hyperparameters\n",
    "        self.window_size = parameters[\"window_size\"]\n",
    "        self.embed_dim = parameters[\"embed_dim\"]\n",
    "        self.hidden_dim = parameters[\"hidden_dim\"]\n",
    "        self.freeze_embeddings = parameters[\"freeze_embeddings\"]\n",
    "        \n",
    "        #create vectors\n",
    "        self.embeds = nn.Embedding(voc_size,self.embed_dim,padding_idx=pad_ix)\n",
    "\n",
    "        if self.freeze_embeddings:\n",
    "            self.embeds.weight.requires_grad = False\n",
    "\n",
    "        # full window size \n",
    "        full_size = 2*window_size+1\n",
    "        self.hidden_layer = nn.Sequential(\n",
    "            nn.Linear(full_size *self.embed_dim,self.hidden_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.output_layer = nn.Linear(self.hidden_dim,1)\n",
    "        self.probabilities = nn.Sigmoid()\n",
    "\n",
    "    def forward(self,input):\n",
    "        B,L = input.size()\n",
    "\n",
    "        token_windows = input.unfold(1,2*self.window_size+1,1)\n",
    "        \n",
    "        _,adjusted_length,_ = token_windows.size()\n",
    "        # Good idea to do internal tensor-size sanity checks, at the least in comments!\n",
    "        assert token_windows.size() == (B, adjusted_length, 2 * self.window_size + 1)\n",
    "\n",
    "        embeded_windows = self.embeds(token_windows)\n",
    "        embeded_windows = embeded_windows.view(B,adjusted_length,-1)\n",
    "\n",
    "        layer_1 = self.hidden_layer(embeded_windows)\n",
    "        output = self.output_layer(layer_1)\n",
    "\n",
    "        output = self.probabilities(output)\n",
    "        output = output.view(B,-1)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_hyperparameters = {\n",
    "    \"batch_size\": 2,\n",
    "    \"window_size\": 2,\n",
    "    \"embed_dim\": 25,\n",
    "    \"hidden_dim\": 25,\n",
    "    \"freeze_embeddings\": False,\n",
    "}\n",
    "voc_size = len(word_to_ix)\n",
    "model = WordWindowClassifer(model_hyperparameters, voc_size)\n",
    "\n",
    "lr1 = 0.05\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr = lr1)\n",
    "\n",
    "# Define a loss function, which computes to binary cross entropy loss\n",
    "def loss_function(batch_outputs, batch_labels, batch_lengths):\n",
    "    # Calculate the loss for the whole batch\n",
    "    bceloss = nn.BCELoss()\n",
    "    loss = bceloss(batch_outputs, batch_labels.float())\n",
    "\n",
    "    # Rescale the loss. Remember that we have used lengths to store the number of words in each training example\n",
    "    loss = loss / batch_lengths.sum().float()  # batch_lengths = the actual length of a sentence\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(loss_function,optimizer,model,loader):\n",
    "    total_loss =0\n",
    "    for batch_inputs,batch_labels,batched_lengths in loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model.forward(batch_inputs)\n",
    "        loss = loss_function(outputs,batch_labels,batched_lengths)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss\n",
    "\n",
    "def train(loss_function,optimizer,model,loader,num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = train_epoch(loss_function,optimizer,model,loader)\n",
    "        if epoch %100 == 0: print(f\"epoch:{epoch} loss:{epoch_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 loss:0.0035\n",
      "epoch:100 loss:0.0034\n",
      "epoch:200 loss:0.0024\n",
      "epoch:300 loss:0.0028\n",
      "epoch:400 loss:0.0022\n",
      "epoch:500 loss:0.0023\n",
      "epoch:600 loss:0.0015\n",
      "epoch:700 loss:0.0017\n",
      "epoch:800 loss:0.0016\n",
      "epoch:900 loss:0.0015\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 1000\n",
    "train(loss_function, optimizer, model, loader, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0, 1]])\n",
      "tensor([[0.0096, 0.0803, 0.0027, 0.9985]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Create test sentences\n",
    "test_corpus = [\"She comes from Paris\"]\n",
    "test_sentences = [s.lower().split() for s in test_corpus]\n",
    "test_labels = [[0, 0, 0, 1]]    #initialize a random label\n",
    "\n",
    "# Create a test loader\n",
    "test_data = list(zip(test_sentences, test_labels))\n",
    "batch_size = 1\n",
    "shuffle = False\n",
    "window_size = 2\n",
    "collate_fn = partial(custom_collate_fn, window_size=2, word_to_ix=word_to_ix)\n",
    "test_loader = torch.utils.data.DataLoader(test_data,batch_size=1,shuffle=False,collate_fn=collate_fn)\n",
    "\n",
    "for test_instance, labels, _ in test_loader:\n",
    "  outputs = model(test_instance)\n",
    "  print(labels)\n",
    "  print(outputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minist",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
